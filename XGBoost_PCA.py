# -*- coding: utf-8 -*-
"""Inter IIT 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CuEx2S8IaFeJEl2oLecveKdRu7mmQZke
"""

!pip install catboost

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import math
import gzip
from sklearn.ensemble import ExtraTreesRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics.scorer import make_scorer
from fancyimpute import IterativeImputer as MICE
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestRegressor
# test = pd.merge(test, test1, on="id",how = 'inner'
from sklearn.linear_model import OrthogonalMatchingPursuit
from sklearn.decomposition import PCA
from catboost import *
pca = PCA(n_components=106)

datas = 'drive/My Drive/'
data = 'drive/My Drive/one_hot_text_features/'
train = pd.read_csv(data + 'text_features_train_idf.csv')
test = pd.read_csv(data + 'text_features_test_idf.csv')
X1 = train.drop(['id'], axis=1)
X2 = test.drop(['id'], axis = 1)
X1 = pca.fit_transform(X1)
X2 = pca.transform(X2)
cols = []
for i in range(0,106):
     cols.append('col' + str(i))
X1 = pd.DataFrame(data = X1, columns = cols)
X2 = pd.DataFrame(data = X2, columns = cols)
test1 = pd.concat([X2, test[['id']]], axis = 1)
train1 = pd.concat([X1, train[['id']]], axis = 1)

train1 = pd.concat([train1,test1])

train1.shape

datas = 'drive/My Drive/'
data_ = pd.read_csv(datas + 'train.csv')
data_test = pd.read_csv(datas + 'test_sort.csv')
cat_jug=pd.read_csv('2900.csv')
data_test['target']=cat_jug['target']
data___target=data_['target']
data_=data_.drop('target',axis=1)
data_['target']=data___target
final_data=pd.concat([data_,data_test])
final_data['span']=final_data['span'].apply(lambda x: x+np.random.rand())

final_data = pd.merge(final_data,train1,on = 'id',how = 'inner')
data_test = pd.merge(data_test,test1,on = 'id',how = 'inner')

final_data.shape

data_test.shape

X_train=final_data.drop(['id','target'],axis=1)
y_train=final_data['target']
X_test=data_test.drop(['id','target'],axis=1)
y_test=data_test['target']
# clf = ExtraTreesRegressor(criterion = 'mse',n_estimators=610)
# clf = clf.fit(X_train, y_train)
# clf.feature_importances_
# model = SelectFromModel(clf, prefit=True)
# X_new = model.transform(X_train)
#X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.33, random_state=42)
xgb = XGBRegressor(objective ='reg:linear', task_type="GPU",eval_metric = 'rmse',colsample_bytree = 0.92, learning_rate = 0.034,max_depth = 5, alpha = 7, n_estimators = 2500)
xgb.fit(X_train, y_train)
# cgb = CatBoostRegressor(learning_rate=0.3,task_type="GPU",verbose=100,use_best_model=True,loss_function='RMSE',iterations=10000)
# cgb.fit(X_train,y_train,eval_set=(X_test,y_test))
# rf = RandomForestRegressor(n_estimators=1600, random_state=0)
# rf.fit(X_train,y_train)

X_test=data_test.drop(['id','target'],axis=1)
data_test.head()

#test_new = model.transform(X_test)
p=xgb.predict(X_test)
id = np.array(data_test['id'])
sub = np.concatenate((np.reshape(id, (1000,1)),np.reshape(p, (1000,1))), axis = 1)
pd.DataFrame(sub).to_csv('output.csv',index=False)

X_train=final_data.drop(['id','target'],axis=1)
y_train=final_data['target']
X_test=data_test.drop(['id','target'],axis=1)
y_test=data_test['target']
clf = ExtraTreesRegressor(criterion = 'mse',n_estimators=1600)
clf = clf.fit(X_train, y_train)
clf.feature_importances_
model = SelectFromModel(clf, prefit=True)
X_new = model.transform(X_train)
# X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.33, random_state=42)
#xgb = XGBRegressor(objective ='reg:linear', eval_metric = 'rmse',colsample_bytree = 0.92, learning_rate = 0.034,max_depth = 5, alpha = 7, n_estimators = 1600)
#xgb.fit(X_new, y_train)
rf = RandomForestRegressor(n_estimators=1600, random_state=0)

data = 'drive/My Drive/'
# text = data + 'text_features/'
train = pd.read_csv(data + 'train.csv')
test = pd.read_csv(data + 'test.csv')
train1 = pd.read_csv(data + 'text_features_idf/text_features_idf_train.csv')
test1 = pd.read_csv(data + 'text_features_idf/text_features_idf_test.csv')
# train1 = pd.read_csv(data + 'train_features.csv')
# test1 = pd.read_csv(data + 'test_features.csv')
train = pd.merge(train,train1, on="id",how = 'inner')
test = pd.merge(test, test1, on="id",how = 'inner')
mu =  train['target'].mean()
sd = train['target'].std()
mu1 = train['span'].mean()
sd1 = train['span'].std()
# train['span'] = (train['span'] - mu1)/sd1
# test['span'] = (test['span'] - mu1)/sd1
# train['target'] = (train['target'] - mu)/sd
#test['target'] = (test['target'] - mu)/sd

for cols in test.columns:
    if(cols != 'id' and cols != 'span' and cols != 'target'):
        df_new = test[test[cols].notnull()]
        col = df_new[cols].astype(float)
        med_col = col.quantile(0.5)
        q1_col = col.quantile(0.25)
        q3_col = col.quantile(0.75)
        ir_col = q3_col - q1_col
        outlier_low = q1_col - 1.5*ir_col
        outlier_high = q3_col + 1.5*ir_col
        df_new[cols] = df_new[cols].astype(float)
        df_col = df_new[(df_new[cols] >= outlier_low) & (df_new[cols] <= outlier_high)]
        col = df_col[cols].astype(float)
        mean_col = np.mean(col)
        test[cols].fillna(mean_col,inplace = True)

for cols in train.columns:
    if(cols != 'id' and cols != 'span' and cols != 'target'):
        df_new = train[train[cols].notnull()]
        col = df_new[cols].astype(float)
        med_col = col.quantile(0.5)
        q1_col = col.quantile(0.25)
        q3_col = col.quantile(0.75)
        ir_col = q3_col - q1_col
        outlier_low = q1_col - 1.5*ir_col
        outlier_high = q3_col + 1.5*ir_col
        df_new[cols] = df_new[cols].astype(float)
        df_col = df_new[(df_new[cols] >= outlier_low) & (df_new[cols] <= outlier_high)]
        col = df_col[cols].astype(float)
        mean_col = np.mean(col)
        train[cols].fillna(mean_col,inplace = True)

for i in range(0,60):
    if(i < 10):
        train['new' + str(i)] = train['feature_0' + str(i) + '_type5']/train['feature_0' + str(i) + '_type4']
        test['new' + str(i)] = test['feature_0' + str(i) + '_type5']/test['feature_0' + str(i) + '_type4']
    else:
        train['new' + str(i)] = train['feature_' + str(i) + '_type5']/train['feature_' + str(i) + '_type4']
        test['new' + str(i)] = test['feature_' + str(i) + '_type5']/test['feature_' + str(i) + '_type4']
    train['new' + str(i)] = (train['new' + str(i)] - train['new' + str(i)].mean())/train['new' + str(i)].std()
    test['new' + str(i)] = (test['new' + str(i)] - test['new' + str(i)].mean())/test['new' + str(i)].std()

train.shape

test.shape

for i in range(0,len(train.columns),30):
    df1 = test.iloc[:,i:i+30]
    print(df1.isna().sum()) 
    print(df1.dtypes)

for cols in train.columns:
    try:
        print(cols)
        print(train['target'].corr(train[cols]))
    except:
        continue

for i,cols in enumerate(train.columns):
    print(cols)
    if(cols != 'id'):
        plt.figure(i)
        sns.boxplot(train[cols])
        plt.show()

X = train.drop(['id', 'target'], axis=1)
y= train['target']
forest = ExtraTreesRegressor(n_estimators=250,random_state=0)
forest.fit(X,y)
importances = forest.feature_importances_

std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]
print("Feature ranking:")

for f in range(X.shape[1]):
    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

# Plot the feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices],
       color="r", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.show()

import os
for ids in train['id']:
    print(ids)
    for file in os.listdir(text):
        if file.endswith(".gz"):
            print(file)
            data = pd.read_csv(text + file, compression='gzip')
            print(data.index[data['id'] == ids].tolist())
        break

from autoimpute.imputations import SingleImputer, MultipleImputer
si = SingleImputer() # imputation methods, passing through the data once
mi = MultipleImputer() # imputation methods, passing through the data multiple times

# train_cols = list(train)
X = train.drop(['id', 'target'], axis=1)
X = MICE().fit_transform(X)

X_test1 = test.drop(['id'], axis = 1)
X_test1 = MICE().fit_transform(X_test)

def scorer(true,pred):
    error = math.sqrt(mean_squared_error(pred,true))
    return math.exp(-1*error)
score = make_scorer(scorer, greater_is_better=True)

X = train.drop(['id', 'target'], axis=1)
y= train['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
# model = XGBRegressor(objective ='reg:linear', eval_metric = 'rmse',colsample_bytree = 0.3, learning_rate = 0.01,
#                 max_depth = 5, alpha = 10, n_estimators = 3400)
# model.fit(X_train, y_train)
# Y_pred = model.predict(X_test)
# print(mean_squared_error(Y_pred, y_test))
# from sklearn.feature_selection import RFECV
# gdb = XGBRegressor(objective ='reg:linear', eval_metric = 'rmse',colsample_bytree = 0.3, learning_rate = 0.01,max_depth = 5, alpha = 10, n_estimators = 3400)
# selector1 = RFECV(gdb,step=1,cv=2,n_jobs =-1,scoring=score,verbose = 2,min_features_to_select = 340)
# #start_time = timer(None)
# selector1.fit(X_train, y_train)
# #timer(start_time)
# selected1 = X_train.columns[selector1.support_]
# print(selected1)
clf = ExtraTreesRegressor(criterion = 'mse',n_estimators=1600)
clf = clf.fit(X, y)
clf.feature_importances_  
model = SelectFromModel(clf, prefit=True)
X_new = model.transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.33, random_state=42)
xgb = XGBRegressor(objective ='reg:linear', eval_metric = 'rmse',colsample_bytree = 0.92, learning_rate = 0.025,
                max_depth = 5, alpha = 7, n_estimators = 4300)
xgb.fit(X_train, y_train)
# reg = OrthogonalMatchingPursuit().fit(X_train, y_train)
# rf = RandomForestRegressor(n_estimators = 160, random_state=0)
xgb.fit(X_train,y_train)

Y_pred = rf.predict(X_test)
print(math.sqrt(mean_squared_error(Y_pred, y_test)))

rf.fit(X_new,y)

test = test.sort_values('id',inplace=False, ascending=False)
test = test.reset_index(drop = True)

X_test.shape

X_train.shape

test.head(5)

test_new = test.drop(['id'], axis=1)
test_new = model.transform(test_new)
lb_pred = rf.predict(test_new)
file = open('output.csv', 'w')
file.write('id,') 
file.write('target')
file.write('\n')
for i in range(0, test.shape[0]):
    print(i)
    file.write(test['id'][i])
    file.write(',')
    file.write(str(lb_pred[i]))
    file.write('\n')

test_new.shape

output = pd.read_csv('output.csv')
output.shape

cols =  list(train.columns)
index_old = [0]
for types in range(0,6):
    print("types",types)
    for i in range(0,60):
        try:
            if(i<10):
                if(types == 0):
                    index = list(train['feature_0' + str(i)].index['feature_0' + str(i)])
                else:
                    index = list(train['feature_0' + str(i) + '_type' + str(types)].index[train['feature_0' + str(i) + '_type' + str(types)].apply(np.isnan)])
            else:
                if(types == 0):
                    index = list(train['feature_' + str(i)].index['feature_' + str(i)])
                else:
                    index = list(train['feature_' + str(i) + '_type' + str(types)].index[train['feature_' + str(i) + '_type' + str(types)].apply(np.isnan)])
            if(index == index_old):
                continue
            else:
                index_old = index
                print(index_old)
                print("feature",i)
        except:
            continue

ids = ["M1ZmcDRnVzdleVFIQnFOSkMI0VW+soe+EcxQ0lxUW40=",
"M25CRlNORHFDdXZFelhyNw7A9lKG6cHAGB0c++yAmhM=",
"MzVTS3RiUVdlVHFJaURkMTYY4OKfZXjOX76bhPBnfBo=",
"Q2N4M21UN080SFNMejB5YUHVwnFME82Vy8UKub3HQEs=",
"RklzZXJWSEpqV000QjhHcX+tLkPLKcb0uv/H98qixEM=",
"SkxLeTZsZndDVVRxcHU1Vstv2wizlROZ728RsxiApzg=",
"SlE2Yk9DNG1VajcyWXhNMJJX3j3FqoGVbalWZUqecTw=",
"T3hzYllNRGVwaGpnb2NDVLR5KRD6h43Z1NgQdeKL6No=",
"TTZXN1NIUmtPelFsSkJnNBwVql+firTYArvPGwVaBU4=",
"TWYwWTRQT2pWMUN5dko1N2TLGW7JivFNhdWAxBd4nws=",
"UDdhZHJsZTVSSlZUeHZMbSGZl1ABLnABGXsNqOYhGsg=",
"UG4yeDRlVG1SeUVwNmNDTtMY8rN1EQ68k6Ac7zSTxJM=",
"UmQ2bnNPWDc4MGFmNWNRbxaonYft85qzjfGJepsqP9M=",
"V3JOOTBERzV6c0hKM1NLNpZezaE59uFTM4JYi/Lg688=",
"V3lnOHE5RlNRTHBQTkdjeBjKDRmSY4Tvbf/gWTLDwT8=",
"VGJLQnUxUHRxWG5FYWd4Y5Wcam/ci7PeifCpxNbxins=",
"Y1lPVURtczAxOWtJZDRDdgiJrm9vFD7Ylih9kPlKpRM=",
"YnRXY2VvVW4zOERkTE51cuKmrkjVvgoLltWGlPCiT24=",
"a051RGUwM2FYeDc5MTZFRk6Gvoo/70rHHKWcgQsdPj0=",
"am5lNHZkc2NyV1AxSHRVeUej0xOucLtLDNZMj7xVB68=",
"bGY5bTZKN0JMSWJnMnhGcBWoTuJ+y5ee7AmSlpn/0eo=",
"dmlrZlhycGxZSzVtT1dGat8KVByi7WZGToem0Cvw1Y4="]

for i,j in enumerate(ids):
    if(i==0):
        df = train[train.id != j]
    else:
        df = df[df.id != j]
train = df

for cols in test.columns:
    if(cols != 'id' and cols != 'span' and cols != 'target'):
        df_new = test[test[cols].notnull()]
        col = df_new[cols].astype(float)
        med_col = col.quantile(0.5)
        q1_col = col.quantile(0.25)
        q3_col = col.quantile(0.75)
        ir_col = q3_col - q1_col
        outlier_low = q1_col - 1.5*ir_col
        outlier_high = q3_col + 1.5*ir_col
        df_new[cols] = df_new[cols].astype(float)
        df_col = df_new[(df_new[cols] >= outlier_low) & (df_new[cols] <= outlier_high)]
        col = df_col[cols].astype(float)
        mean_col = np.mean(col)
        test[cols].fillna(mean_col,inplace = True)







